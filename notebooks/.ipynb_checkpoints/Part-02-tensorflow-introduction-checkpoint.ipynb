{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximating functions with deep ReLU networks\n",
    "*Practical session for the course [Mathematic of deep learning](http://www.lsf.tu-berlin.de/qisserver/servlet/de.his.servlet.RequestDispatcherServlet?state=verpublish&status=init&vmfile=no&publishid=233449&moduleCall=webInfo&publishConfFile=webInfo&publishSubDir=veranstaltung), October 2019.*\n",
    "The content is mostly based on [D. Yarotsky, 2017](https://www.sciencedirect.com/science/article/pii/S0893608017301545).\n",
    "\n",
    "## Part II: Getting started with Tensorflow\n",
    "\n",
    "In this part we will introduce the basic concepts of using Tensorflow. This is not yet related to the approximation theoretic results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the core of Tensorflow are the **tensors**, which can be thought of as multi-dimensional data-containers.\n",
    "* 1D tensor represents for example a **vector** or a collection of scalars.\n",
    "* a 2D tensor represents for example a **matrix** or a collection of vectors.\n",
    "* and so on...\n",
    "\n",
    "The **shape** of a tensor is the number of components in each of its dimensions, for example a tensor representing a $5\\times 4$ matrix has shape $[5, 4]$.\n",
    "\n",
    "There are several types of tensors in Tensorflow:\n",
    "* **constant** tensors (these have defined shape and values and are not changed after creation)\n",
    "* **variable** tensors (these have defined shape and initial values but can be changed after creation, for example used for the weights of a neural network)\n",
    "* **placeholder** tensors (these only have a shape but do not contain any data (yet) and are used to serve as placeholders in computations where the data is provided later, for example network inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# create a constant scalar (0D) tensor with value 5\n",
    "constant_scalar = tf.constant(5.0)\n",
    "print(constant_scalar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const_1:0\", shape=(3, 2), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "# create a constant matrix (2D) tensor with random values\n",
    "constant_matrix = tf.constant(np.random.randn(3,2))\n",
    "print(constant_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the creation of a tensor its **data type** can be specified by providing the `dtype` argument. For example we can create floating point tensors using 32-bit single precision instead of 64-bit double precision arithmetic (which is usually accurate enough for many applications but decreases computational costs). We can also give **names** to tensors to make it easier to identify them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer=tf.contrib.layers.xavier_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function tensorflow.contrib.layers.python.layers.initializers.xavier_initializer(uniform=True, seed=None, dtype=tf.float32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.contrib.layers.xavier_initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'my_matrix_2:0' shape=(3, 2) dtype=float32_ref>\n",
      "<tf.Variable 'my_vector_1:0' shape=(3,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "# create a variable matrix (2D) tensor with random inital values\n",
    "variable_matrix = tf.Variable(initializer((3,2)), dtype=tf.float32, name='my_matrix')\n",
    "print(variable_matrix)\n",
    "\n",
    "variable_vector = tf.Variable(initializer((3,)), dtype=tf.float32, name='my_vector')\n",
    "print(variable_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors can be transformed into other tensors by so called **operations** (ops). For example we can define another tensor, for example an input placeholder, and compute a new tensor using the **matrix multiplication** and **vector addition** operations.\n",
    "\n",
    "However, note that **matrix multiplication** requires two matrices, so we have to **reshape** the vector tensor into a matrix with a single column. After the mutiplication we can reshape the result back into a vector.\n",
    "\n",
    "Finally we can apply a componentwise non-linear function, for example the ReLU. We have then coded our first neural network layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"my_input_2:0\", shape=(2,), dtype=float32)\n",
      "Tensor(\"Reshape_5:0\", shape=(3,), dtype=float32)\n",
      "Tensor(\"add_2:0\", shape=(3,), dtype=float32)\n",
      "Tensor(\"Relu_2:0\", shape=(3,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "placeholder_input = tf.placeholder(dtype=tf.float32, shape=(2,), name='my_input')\n",
    "print(placeholder_input)\n",
    "matrix_product = tf.reshape(tf.matmul(variable_matrix, tf.reshape(placeholder_input, [2, 1])), [3])\n",
    "print(matrix_product)\n",
    "vector_sum = matrix_product + variable_vector\n",
    "print(vector_sum)\n",
    "relu_vector = tf.nn.relu(vector_sum)\n",
    "print(relu_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay this is nice, but we want to see the actual values of the tensors, not only the shape and name. We can not do this yet, because all we have done so far is building the so called **computation graph**, which tells Tensorflow how tensors are related. But no actual computations have been done, which in fact is not even possible yet, since Tensorflow does not know the values of our placeholder vector. So we have to do three things:\n",
    "1. Start a Tensorflow **session**, which is used to manage the actual computations (for example determine which computations should be done on which hardware device in case you have multiple devices like CPU cores and GPU).\n",
    "2. Initialize all tensors in the computation graph. \n",
    "2. Let the session run certain computations, by specifying which output tensors we would like to compute, and **feeding** in all placeholder values necessary to execute the computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0., 0., 0.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "session = tf.Session()\n",
    "session.run(tf.global_variables_initializer())\n",
    "relu_vector_value = session.run([relu_vector], feed_dict={placeholder_input: np.asarray([1, 2])})\n",
    "print(relu_vector_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to **train** the variable tensors in our network, we need to define a second placeholder for the desired output values, a **loss** function that we wish to minimize, and an **optimizer**, for example a gradient descent method. As before, we wil first define the operations as part of the computation graph and then let the session run these operations to take an effect.\n",
    "\n",
    "Let us try to train our single layer network to learn the very simple map \n",
    "\n",
    "$$ \\begin{pmatrix} x_1\\\\ x_2 \\end{pmatrix} \\mapsto \\begin{pmatrix} \\mathrm{relu}(x_1) \\\\ 0 \\\\ 0 \\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter    \tloss    \n",
      "--------\t--------\n",
      "       0\t0.00e+00\n",
      "       1\t0.00e+00\n",
      "       2\t5.87e-01\n",
      "       3\t0.00e+00\n",
      "       4\t0.00e+00\n",
      "       5\t1.42e+00\n",
      "       6\t0.00e+00\n",
      "       7\t1.28e-01\n",
      "       8\t0.00e+00\n",
      "       9\t0.00e+00\n"
     ]
    }
   ],
   "source": [
    "placeholder_output = tf.placeholder(dtype=tf.float32, shape=[3], name='my_output')\n",
    "loss = tf.reduce_sum(tf.square(relu_vector-placeholder_output))\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.1)\n",
    "gradient_descent_step = optimizer.minimize(loss)\n",
    "\n",
    "# run some gradient descent steps with random inputs and observe the loss function\n",
    "print('{:8s}\\t{:8s}\\n{:8s}\\t{:8s}'.format('iter', 'loss', '--------', '--------'))\n",
    "for i in range(10):\n",
    "    in_vec = np.random.randn(2)\n",
    "    out_vec = np.maximum(0, np.append(in_vec, 0))*[1, 0, 0]\n",
    "    _, loss_value = session.run(\n",
    "        [gradient_descent_step, loss], \n",
    "        feed_dict={placeholder_input: in_vec, placeholder_output:out_vec }\n",
    "    )\n",
    "    print('{:8d}\\t{:1.2e}'.format(i, loss_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see what the network has **learned** and retrieve the values of the variable tensors. Note how we do not have to feed values for the placeholder vectors this time, as no computations involving them have to be computed now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.02133051 -0.5428346 ]\n",
      " [ 0.04250121 -0.263022  ]\n",
      " [-0.16397363  0.00383917]]\n",
      "[-0.8984948  -0.62784505 -0.31619728]\n"
     ]
    }
   ],
   "source": [
    "matrix_value, vector_value = session.run([variable_matrix, variable_vector])\n",
    "print(matrix_value)\n",
    "print(vector_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In most cases we will not feed single input vectors to a call of `session.run()` but always collections of multiple input vectors, called batches. It is customary to reserve the first dimension of all tensors for the batch index. Hence, if you build Tensorflow operations always make sure that your code can handle tensors with an additional first dimension that is passed on unaltered. If the size of batches is not known before, Tensorflow allows tensors to have an undetermined shape in some dimensions.\n",
    "\n",
    "For example the placeholder tensor below can be fed with a collection of 5 dimensional vectors, however the number of vectors can vary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder_1:0\", shape=(?, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "batched_vectors = tf.placeholder(dtype=tf.float32, shape=[None, 5])\n",
    "print(batched_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For various frequently used operations Tensorflow provides shortcut aliases. For example the single ReLU layer that we have created above can be simply defined using the `dense` operation which creates a fully connected layer. In that case we do not have to worry about creating the variable tensors (this will be done automatically by Tensorflow \"under the hood\"). This simplicity comes at the cost of less flexibility though, as we do not have direct access to the variable tensors. So the best way to define a network is a matter of personal preference and depends on the application...there is usually more than one way to do something in Tenorflow.\n",
    "\n",
    "Have a look around the [Tensorflow Documentation](https://www.tensorflow.org/api_docs/python/tf) to see what other operations are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"dense_2/Relu:0\", shape=(1, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "alternative_relu_vector = tf.layers.dense(tf.reshape(placeholder_input, [-1, 2]), 3, activation=tf.nn.relu)\n",
    "print(alternative_relu_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tomography",
   "language": "python",
   "name": "tomography"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
